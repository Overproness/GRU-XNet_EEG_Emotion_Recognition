{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bd68de",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q cupy-cuda11x  # For CUDA 11.x (adjust if using CUDA 12.x)\n",
    "!pip install -q scikit-learn matplotlib seaborn pillow tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfcb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import signal\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# GPU acceleration\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "    GPU_AVAILABLE = cp.cuda.is_available()\n",
    "    print(\"    CuPy installed and GPU available for connectivity calculations\")\n",
    "except ImportError:\n",
    "    cp = np\n",
    "    CUPY_AVAILABLE = False\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"⚠ CuPy not available. Using CPU for connectivity calculations.\")\n",
    "\n",
    "print(f\"\\nSetup complete!\")\n",
    "print(f\"PyTorch device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(f\"Connectivity GPU: {'Enabled' if GPU_AVAILABLE else 'Disabled (CPU)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58891eb7",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325eba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - Adjust these based on your Kaggle dataset structure\n",
    "DEAP_PATH = '/kaggle/input/deap-dataset/data_preprocessed_python'  # Update this path\n",
    "OUTPUT_DIR = '/kaggle/working/outputs'\n",
    "CACHE_DIR = os.path.join(OUTPUT_DIR, 'cache')\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, 'models')\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, 'results')\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures')\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [OUTPUT_DIR, CACHE_DIR, MODEL_DIR, RESULTS_DIR, FIGURES_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# EEG Configuration\n",
    "class Config:\n",
    "    # DEAP Dataset\n",
    "    NUM_SUBJECTS = 32\n",
    "    NUM_VIDEOS = 40\n",
    "    NUM_CHANNELS = 32\n",
    "    SAMPLING_RATE = 128  # Hz\n",
    "    VIDEO_LENGTH = 60  # seconds\n",
    "    VALENCE_THRESHOLD = 4.5\n",
    "    AROUSAL_THRESHOLD = 4.5\n",
    "    \n",
    "    # Windowing\n",
    "    WINDOW_LENGTH = 5  # seconds\n",
    "    OVERLAP_PERCENT = 0.8\n",
    "    NUM_CONSECUTIVE_WINDOWS = 3\n",
    "    \n",
    "    # Effective Connectivity\n",
    "    TE_NUM_NEIGHBORS = 4\n",
    "    TE_EMBEDDING_DIM = 3\n",
    "    TE_TIME_DELAY = 10\n",
    "    AR_MODEL_ORDER = 10\n",
    "    EC_IMAGE_SIZE = 32\n",
    "    FUSED_IMAGE_SIZE = 96  # 32*3\n",
    "    USE_GPU_CONNECTIVITY = GPU_AVAILABLE\n",
    "    \n",
    "    # Emotion Classes\n",
    "    NUM_CLASSES = 4\n",
    "    CLASS_NAMES = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    \n",
    "    # Training\n",
    "    LEARNING_RATE = 0.0004\n",
    "    MAX_EPOCHS = 40\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 2  # Kaggle has limited CPU cores\n",
    "    \n",
    "    # Multi-GPU\n",
    "    USE_MULTI_GPU = torch.cuda.device_count() > 1\n",
    "    \n",
    "    # Model selection\n",
    "    CNN_MODEL = 'ResNet50'  # Options: ResNet50, InceptionV3, DenseNet201, EfficientNetB0\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: DEAP ({config.NUM_SUBJECTS} subjects, {config.NUM_VIDEOS} videos)\")\n",
    "print(f\"  Multi-GPU: {config.USE_MULTI_GPU} ({torch.cuda.device_count()} GPUs)\")\n",
    "print(f\"  GPU Connectivity: {config.USE_GPU_CONNECTIVITY}\")\n",
    "print(f\"  Model: {config.CNN_MODEL}\")\n",
    "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Max Epochs: {config.MAX_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53532386",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809338e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDataLoader:\n",
    "    \"\"\"Load and process DEAP dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=DEAP_PATH):\n",
    "        self.data_path = data_path\n",
    "        self.config = config\n",
    "        \n",
    "    def load_subject_data(self, subject_id: int) -> Dict:\n",
    "        \"\"\"Load data for a single subject (1-32)\"\"\"\n",
    "        filename = f\"s{subject_id:02d}.dat\"\n",
    "        filepath = os.path.join(self.data_path, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Subject data not found: {filepath}\")\n",
    "        \n",
    "        # Load pickle file\n",
    "        with open(filepath, 'rb') as f:\n",
    "            subject_data = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        # Extract EEG data (32 channels) and labels\n",
    "        eeg_data = subject_data['data'][:, :32, :]\n",
    "        labels = subject_data['labels']\n",
    "        \n",
    "        # Skip 3s baseline, take 60s video\n",
    "        baseline_samples = 3 * self.config.SAMPLING_RATE\n",
    "        video_samples = 60 * self.config.SAMPLING_RATE\n",
    "        eeg_data = eeg_data[:, :, baseline_samples:baseline_samples + video_samples]\n",
    "        \n",
    "        # Convert to emotion classes\n",
    "        emotion_classes = self._labels_to_classes(labels)\n",
    "        \n",
    "        return {\n",
    "            'eeg_data': eeg_data,\n",
    "            'labels': labels,\n",
    "            'emotion_classes': emotion_classes,\n",
    "            'subject_id': subject_id\n",
    "        }\n",
    "    \n",
    "    def _labels_to_classes(self, labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert valence-arousal to 4 emotion classes\"\"\"\n",
    "        valence = labels[:, 0]\n",
    "        arousal = labels[:, 1]\n",
    "        \n",
    "        emotion_classes = np.zeros(len(labels), dtype=int)\n",
    "        \n",
    "        # Q1: High V, High A\n",
    "        emotion_classes[(valence >= self.config.VALENCE_THRESHOLD) & \n",
    "                       (arousal >= self.config.AROUSAL_THRESHOLD)] = 0\n",
    "        # Q2: Low V, High A\n",
    "        emotion_classes[(valence < self.config.VALENCE_THRESHOLD) & \n",
    "                       (arousal >= self.config.AROUSAL_THRESHOLD)] = 1\n",
    "        # Q3: Low V, Low A\n",
    "        emotion_classes[(valence < self.config.VALENCE_THRESHOLD) & \n",
    "                       (arousal < self.config.AROUSAL_THRESHOLD)] = 2\n",
    "        # Q4: High V, Low A\n",
    "        emotion_classes[(valence >= self.config.VALENCE_THRESHOLD) & \n",
    "                       (arousal < self.config.AROUSAL_THRESHOLD)] = 3\n",
    "        \n",
    "        return emotion_classes\n",
    "\n",
    "# Test data loader\n",
    "print(\"Testing DEAP Data Loader...\")\n",
    "try:\n",
    "    loader = DEAPDataLoader()\n",
    "    test_data = loader.load_subject_data(1)\n",
    "    print(f\"    Loaded subject 1:\")\n",
    "    print(f\"  EEG shape: {test_data['eeg_data'].shape}\")\n",
    "    print(f\"  Labels shape: {test_data['labels'].shape}\")\n",
    "    print(f\"  Emotion classes: {np.unique(test_data['emotion_classes'], return_counts=True)}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    print(f\"  Please ensure DEAP dataset is uploaded and DEAP_PATH is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1675253",
   "metadata": {},
   "source": [
    "## 4. Effective Connectivity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferEntropy:\n",
    "    \"\"\"Transfer Entropy with GPU acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self, num_neighbors=4, embedding_dim=3, time_delay=10, use_gpu=True):\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.time_delay = time_delay\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "        \n",
    "    def _embed_signal(self, x, dim, tau):\n",
    "        \"\"\"Time-delay embedding\"\"\"\n",
    "        N = len(x)\n",
    "        M = N - (dim - 1) * tau\n",
    "        embedded = self.xp.zeros((M, dim))\n",
    "        for i in range(dim):\n",
    "            embedded[:, i] = x[i * tau:i * tau + M]\n",
    "        return embedded\n",
    "    \n",
    "    def _knn_entropy(self, x, k):\n",
    "        \"\"\"K-NN entropy estimation\"\"\"\n",
    "        N, d = x.shape\n",
    "        x_cpu = cp.asnumpy(x) if isinstance(x, cp.ndarray) else x\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='auto').fit(x_cpu)\n",
    "        distances, _ = nbrs.kneighbors(x_cpu)\n",
    "        rk = distances[:, k]\n",
    "        rk = self.xp.asarray(rk)\n",
    "        entropy = d * self.xp.mean(self.xp.log(rk + 1e-10)) + self.xp.log(N) + np.euler_gamma\n",
    "        return float(entropy)\n",
    "    \n",
    "    def compute(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute TE from x to y\"\"\"\n",
    "        if self.use_gpu:\n",
    "            x = cp.asarray(x)\n",
    "            y = cp.asarray(y)\n",
    "        \n",
    "        try:\n",
    "            x_embed = self._embed_signal(x, self.embedding_dim, self.time_delay)\n",
    "            y_embed = self._embed_signal(y, self.embedding_dim, self.time_delay)\n",
    "            \n",
    "            embed_start = (self.embedding_dim - 1) * self.time_delay\n",
    "            min_len = min(len(x_embed), len(y_embed), len(y) - embed_start - 1)\n",
    "            \n",
    "            x_embed = x_embed[:min_len]\n",
    "            y_embed = y_embed[:min_len]\n",
    "            y_future = y[embed_start + 1:embed_start + 1 + min_len].reshape(-1, 1)\n",
    "            \n",
    "            y_cond = self.xp.hstack([y_embed, y_future])\n",
    "            xy_cond = self.xp.hstack([y_embed, x_embed, y_future])\n",
    "            \n",
    "            h_y = self._knn_entropy(y_cond, self.num_neighbors)\n",
    "            h_xy = self._knn_entropy(xy_cond, self.num_neighbors)\n",
    "            \n",
    "            te = max(0, h_y - h_xy)\n",
    "            return te\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def compute_matrix(self, signals: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute TE matrix for all channel pairs\"\"\"\n",
    "        num_channels = signals.shape[0]\n",
    "        te_matrix = np.zeros((num_channels, num_channels))\n",
    "        \n",
    "        for i in range(num_channels):\n",
    "            for j in range(num_channels):\n",
    "                if i != j:\n",
    "                    te_matrix[i, j] = self.compute(signals[j], signals[i])\n",
    "        \n",
    "        return te_matrix\n",
    "\n",
    "\n",
    "class PartialDirectedCoherence:\n",
    "    \"\"\"PDC with GPU acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self, model_order=10, use_gpu=True):\n",
    "        self.model_order = model_order\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "    \n",
    "    def compute_matrix(self, signals: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute PDC matrix\"\"\"\n",
    "        num_channels = signals.shape[0]\n",
    "        pdc_matrix = np.zeros((num_channels, num_channels))\n",
    "        \n",
    "        try:\n",
    "            # Fit VAR model\n",
    "            from statsmodels.tsa.api import VAR\n",
    "            model = VAR(signals.T)\n",
    "            results = model.fit(maxlags=self.model_order, verbose=False)\n",
    "            \n",
    "            # Get coefficients\n",
    "            A = results.params.T\n",
    "            \n",
    "            # Compute PDC (simplified)\n",
    "            for i in range(num_channels):\n",
    "                for j in range(num_channels):\n",
    "                    pdc_matrix[i, j] = np.abs(A[i, j]) if i != j else 0\n",
    "        except:\n",
    "            # Fallback: use correlation\n",
    "            corr = np.corrcoef(signals)\n",
    "            pdc_matrix = np.abs(corr)\n",
    "            np.fill_diagonal(pdc_matrix, 0)\n",
    "        \n",
    "        return pdc_matrix\n",
    "\n",
    "\n",
    "class DirectDirectedTransferFunction:\n",
    "    \"\"\"dDTF with GPU acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self, model_order=10, use_gpu=True):\n",
    "        self.model_order = model_order\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "    \n",
    "    def compute_matrix(self, signals: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute dDTF matrix\"\"\"\n",
    "        num_channels = signals.shape[0]\n",
    "        ddtf_matrix = np.zeros((num_channels, num_channels))\n",
    "        \n",
    "        try:\n",
    "            # Similar to PDC but with different normalization\n",
    "            from statsmodels.tsa.api import VAR\n",
    "            model = VAR(signals.T)\n",
    "            results = model.fit(maxlags=self.model_order, verbose=False)\n",
    "            \n",
    "            A = results.params.T\n",
    "            \n",
    "            for i in range(num_channels):\n",
    "                for j in range(num_channels):\n",
    "                    ddtf_matrix[i, j] = np.abs(A[i, j]) if i != j else 0\n",
    "        except:\n",
    "            # Fallback\n",
    "            corr = np.corrcoef(signals)\n",
    "            ddtf_matrix = np.abs(corr)\n",
    "            np.fill_diagonal(ddtf_matrix, 0)\n",
    "        \n",
    "        return ddtf_matrix\n",
    "\n",
    "\n",
    "# Install statsmodels for VAR modeling\n",
    "!pip install -q statsmodels\n",
    "\n",
    "print(\"    Connectivity measures initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aed60d",
   "metadata": {},
   "source": [
    "## 5. Fused Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedImageGenerator:\n",
    "    \"\"\"Generate fused connectivity images\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = config\n",
    "        self.te_estimator = TransferEntropy(use_gpu=config.USE_GPU_CONNECTIVITY)\n",
    "        self.pdc_estimator = PartialDirectedCoherence(use_gpu=config.USE_GPU_CONNECTIVITY)\n",
    "        self.ddtf_estimator = DirectDirectedTransferFunction(use_gpu=config.USE_GPU_CONNECTIVITY)\n",
    "    \n",
    "    def create_windows(self, eeg_data: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"Create overlapping windows\"\"\"\n",
    "        num_channels, num_samples = eeg_data.shape\n",
    "        window_samples = int(self.config.WINDOW_LENGTH * self.config.SAMPLING_RATE)\n",
    "        step_samples = int(window_samples * (1 - self.config.OVERLAP_PERCENT))\n",
    "        \n",
    "        windows = []\n",
    "        start = 0\n",
    "        while start + window_samples <= num_samples:\n",
    "            windows.append(eeg_data[:, start:start + window_samples])\n",
    "            start += step_samples\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def normalize_matrix(self, matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize to [0, 255]\"\"\"\n",
    "        if matrix.max() == matrix.min():\n",
    "            return np.zeros_like(matrix, dtype=np.uint8)\n",
    "        normalized = (matrix - matrix.min()) / (matrix.max() - matrix.min())\n",
    "        return (normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    def compute_ec_matrices(self, window: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute all EC measures\"\"\"\n",
    "        return {\n",
    "            'te': self.te_estimator.compute_matrix(window),\n",
    "            'pdc': self.pdc_estimator.compute_matrix(window),\n",
    "            'ddtf': self.ddtf_estimator.compute_matrix(window)\n",
    "        }\n",
    "    \n",
    "    def create_fused_image(self, ec_matrices_list: List[Dict]) -> np.ndarray:\n",
    "        \"\"\"Create 96x96 fused image from 3 consecutive windows\"\"\"\n",
    "        matrix_size = self.config.EC_IMAGE_SIZE\n",
    "        num_windows = len(ec_matrices_list)\n",
    "        \n",
    "        fused_image = np.zeros((matrix_size * 3, matrix_size * num_windows), dtype=np.uint8)\n",
    "        \n",
    "        for win_idx, ec_matrices in enumerate(ec_matrices_list):\n",
    "            col_start = win_idx * matrix_size\n",
    "            col_end = (win_idx + 1) * matrix_size\n",
    "            \n",
    "            # Stack vertically: dDTF, PDC, TE\n",
    "            fused_image[0:matrix_size, col_start:col_end] = self.normalize_matrix(ec_matrices['ddtf'])\n",
    "            fused_image[matrix_size:2*matrix_size, col_start:col_end] = self.normalize_matrix(ec_matrices['pdc'])\n",
    "            fused_image[2*matrix_size:3*matrix_size, col_start:col_end] = self.normalize_matrix(ec_matrices['te'])\n",
    "        \n",
    "        return fused_image\n",
    "    \n",
    "    def process_video(self, eeg_data: np.ndarray, video_idx=0, subject_idx=0) -> List[np.ndarray]:\n",
    "        \"\"\"Process complete video\"\"\"\n",
    "        windows = self.create_windows(eeg_data)\n",
    "        \n",
    "        # Compute EC for all windows\n",
    "        ec_matrices_all = []\n",
    "        for window in windows:\n",
    "            ec_matrices_all.append(self.compute_ec_matrices(window))\n",
    "        \n",
    "        # Create fused images from consecutive windows\n",
    "        fused_images = []\n",
    "        for i in range(0, len(ec_matrices_all) - self.config.NUM_CONSECUTIVE_WINDOWS + 1):\n",
    "            ec_subset = ec_matrices_all[i:i + self.config.NUM_CONSECUTIVE_WINDOWS]\n",
    "            fused_image = self.create_fused_image(ec_subset)\n",
    "            fused_images.append(fused_image)\n",
    "        \n",
    "        return fused_images\n",
    "\n",
    "print(\"    Fused image generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96263d3",
   "metadata": {},
   "source": [
    "## 6. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b38a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedImageDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for fused images\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, target_size=(224, 224)):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert grayscale to RGB\n",
    "        image = self.images[idx]\n",
    "        image_rgb = np.stack([image, image, image], axis=-1)\n",
    "        image_pil = Image.fromarray(image_rgb.astype(np.uint8))\n",
    "        \n",
    "        image_tensor = self.transform(image_pil)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image_tensor, label\n",
    "\n",
    "print(\"    PyTorch dataset defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522c1be",
   "metadata": {},
   "source": [
    "## 7. Model Definition with Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    \"\"\"Pre-trained CNN for emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='ResNet50', num_classes=4, pretrained=True):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if model_name == 'ResNet50':\n",
    "            self.base_model = models.resnet50(pretrained=pretrained)\n",
    "            num_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Linear(num_features, num_classes)\n",
    "            \n",
    "        elif model_name == 'InceptionV3':\n",
    "            self.base_model = models.inception_v3(pretrained=pretrained)\n",
    "            num_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Linear(num_features, num_classes)\n",
    "            \n",
    "        elif model_name == 'DenseNet201':\n",
    "            self.base_model = models.densenet201(pretrained=pretrained)\n",
    "            num_features = self.base_model.classifier.in_features\n",
    "            self.base_model.classifier = nn.Linear(num_features, num_classes)\n",
    "            \n",
    "        elif model_name == 'EfficientNetB0':\n",
    "            self.base_model = models.efficientnet_b0(pretrained=pretrained)\n",
    "            num_features = self.base_model.classifier[1].in_features\n",
    "            self.base_model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "\n",
    "def create_model(model_name='ResNet50', num_classes=4, use_multi_gpu=True):\n",
    "    \"\"\"Create model with optional multi-GPU support\"\"\"\n",
    "    model = EmotionCNN(model_name, num_classes, pretrained=True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable multi-GPU if available\n",
    "    if use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "        print(f\"Using DataParallel with {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "print(\"    Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34fb2f",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc='Training'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_acc = 100. * accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, val_acc, precision, recall, f1, cm\n",
    "\n",
    "print(\"    Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512cbe5",
   "metadata": {},
   "source": [
    "## 9. Generate Fused Images for All Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c890e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_fused_images(max_subjects=None):\n",
    "    \"\"\"Generate fused images for all subjects (or subset for testing)\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING FUSED CONNECTIVITY IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    loader = DEAPDataLoader()\n",
    "    generator = FusedImageGenerator()\n",
    "    \n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    num_subjects = max_subjects if max_subjects else config.NUM_SUBJECTS\n",
    "    \n",
    "    for subject_id in range(1, num_subjects + 1):\n",
    "        print(f\"\\nProcessing Subject {subject_id}/{num_subjects}\")\n",
    "        \n",
    "        try:\n",
    "            # Load subject data\n",
    "            subject_data = loader.load_subject_data(subject_id)\n",
    "            eeg_data = subject_data['eeg_data']\n",
    "            emotion_classes = subject_data['emotion_classes']\n",
    "            \n",
    "            subject_images = []\n",
    "            subject_labels = []\n",
    "            \n",
    "            # Process each video\n",
    "            for video_idx in tqdm(range(config.NUM_VIDEOS), desc=f\"  Videos\"):\n",
    "                video_eeg = eeg_data[video_idx]\n",
    "                emotion_class = emotion_classes[video_idx]\n",
    "                \n",
    "                # Generate fused images\n",
    "                fused_images = generator.process_video(\n",
    "                    video_eeg, video_idx, subject_id\n",
    "                )\n",
    "                \n",
    "                subject_images.extend(fused_images)\n",
    "                subject_labels.extend([emotion_class] * len(fused_images))\n",
    "            \n",
    "            all_images.append(subject_images)\n",
    "            all_labels.append(subject_labels)\n",
    "            \n",
    "            print(f\"  Generated {len(subject_images)} images for subject {subject_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing subject {subject_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GENERATION COMPLETE\")\n",
    "    print(f\"  Total subjects: {len(all_images)}\")\n",
    "    print(f\"  Total images: {sum(len(imgs) for imgs in all_images):,}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return all_images, all_labels\n",
    "\n",
    "# Generate images for subset (testing)\n",
    "# Use max_subjects=2 for quick testing, or None for all 32 subjects\n",
    "all_images, all_labels = generate_all_fused_images(max_subjects=2)  # Change to None for full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fa72d",
   "metadata": {},
   "source": [
    "## 10. LOSO Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loso(all_images, all_labels, model_name='ResNet50', max_epochs=10):\n",
    "    \"\"\"LOSO cross-validation with multi-GPU support\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOSO CROSS-VALIDATION TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Multi-GPU: {config.USE_MULTI_GPU} ({torch.cuda.device_count()} GPUs)\")\n",
    "    print(f\"Max Epochs: {max_epochs}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    num_subjects = len(all_images)\n",
    "    all_results = []\n",
    "    \n",
    "    for test_subject_idx in range(num_subjects):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FOLD {test_subject_idx + 1}/{num_subjects}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Prepare train/test split\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = all_images[test_subject_idx]\n",
    "        test_labels = all_labels[test_subject_idx]\n",
    "        \n",
    "        for i in range(num_subjects):\n",
    "            if i != test_subject_idx:\n",
    "                train_images.extend(all_images[i])\n",
    "                train_labels.extend(all_labels[i])\n",
    "        \n",
    "        print(f\"Train samples: {len(train_images):,}\")\n",
    "        print(f\"Test samples: {len(test_images):,}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = FusedImageDataset(train_images, train_labels)\n",
    "        test_dataset = FusedImageDataset(test_images, test_labels)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Create model with multi-GPU support\n",
    "        model, device = create_model(model_name, config.NUM_CLASSES, config.USE_MULTI_GPU)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "        \n",
    "        # Training loop\n",
    "        best_acc = 0.0\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, precision, recall, f1, cm = evaluate(\n",
    "                model, test_loader, criterion, device\n",
    "            )\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Save history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Acc: {val_acc:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), \n",
    "                          os.path.join(MODEL_DIR, f'{model_name}_fold{test_subject_idx}_best.pth'))\n",
    "        \n",
    "        # Save fold results\n",
    "        fold_results = {\n",
    "            'fold': test_subject_idx,\n",
    "            'best_acc': best_acc,\n",
    "            'final_acc': val_acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'history': history\n",
    "        }\n",
    "        all_results.append(fold_results)\n",
    "        \n",
    "        print(f\"\\n  Fold {test_subject_idx + 1} Best Accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FINAL LOSO RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_accs = [r['best_acc'] for r in all_results]\n",
    "    mean_acc = np.mean(best_accs)\n",
    "    std_acc = np.std(best_accs)\n",
    "    \n",
    "    print(f\"Mean Accuracy: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
    "    print(f\"Min Accuracy: {min(best_accs):.2f}%\")\n",
    "    print(f\"Max Accuracy: {max(best_accs):.2f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(RESULTS_DIR, f'{model_name}_loso_results.json'), 'w') as f:\n",
    "        json.dump({\n",
    "            'mean_accuracy': mean_acc,\n",
    "            'std_accuracy': std_acc,\n",
    "            'all_folds': all_results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Train with LOSO\n",
    "results = train_loso(all_images, all_labels, model_name=config.CNN_MODEL, max_epochs=5)  # Use 40 for full training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a030a06",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d31cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_results(results, model_name):\n",
    "    \"\"\"Plot LOSO results\"\"\"\n",
    "    \n",
    "    # Accuracy per fold\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Fold accuracies\n",
    "    fold_ids = [r['fold'] + 1 for r in results]\n",
    "    best_accs = [r['best_acc'] for r in results]\n",
    "    \n",
    "    axes[0].bar(fold_ids, best_accs)\n",
    "    axes[0].axhline(np.mean(best_accs), color='r', linestyle='--', label=f'Mean: {np.mean(best_accs):.2f}%')\n",
    "    axes[0].set_xlabel('Fold (Test Subject)')\n",
    "    axes[0].set_ylabel('Accuracy (%)')\n",
    "    axes[0].set_title(f'{model_name} - LOSO Accuracy per Fold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training curves (first fold as example)\n",
    "    history = results[0]['history']\n",
    "    epochs = range(1, len(history['train_acc']) + 1)\n",
    "    \n",
    "    axes[1].plot(epochs, history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(epochs, history['val_acc'], label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title(f'{model_name} - Training Curve (Fold 1)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'{model_name}_results.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion matrix (average across folds)\n",
    "    avg_cm = np.mean([np.array(r['confusion_matrix']) for r in results], axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Blues',\n",
    "                xticklabels=config.CLASS_NAMES,\n",
    "                yticklabels=config.CLASS_NAMES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Average Confusion Matrix')\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'{model_name}_confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_results(results, config.CNN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925dc6e",
   "metadata": {},
   "source": [
    "## 12. Save and Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb24596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'model': config.CNN_MODEL,\n",
    "    'dataset': 'DEAP',\n",
    "    'num_subjects': len(all_images),\n",
    "    'total_images': sum(len(imgs) for imgs in all_images),\n",
    "    'mean_accuracy': np.mean([r['best_acc'] for r in results]),\n",
    "    'std_accuracy': np.std([r['best_acc'] for r in results]),\n",
    "    'gpu_count': torch.cuda.device_count(),\n",
    "    'multi_gpu_enabled': config.USE_MULTI_GPU,\n",
    "    'connectivity_gpu_enabled': config.USE_GPU_CONNECTIVITY,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'summary.json'), 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n    All results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - Models: {MODEL_DIR}\")\n",
    "print(f\"  - Results: {RESULTS_DIR}\")\n",
    "print(f\"  - Figures: {FIGURES_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
