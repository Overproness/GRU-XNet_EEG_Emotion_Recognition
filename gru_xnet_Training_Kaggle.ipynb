{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fc23ace",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "43de04ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2.4.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2022.3.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install scipy scikit-learn matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9738eba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from scipy import signal\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import math\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ab8736b",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455e0046",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class STFTConfig:\n",
        "    \"\"\"STFT preprocessing configuration\"\"\"\n",
        "    freq_range: Tuple[float, float] = (0.5, 50.0)\n",
        "    target_freq_bins: int = 129\n",
        "    target_time_bins: int = 126\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model architecture configuration\"\"\"\n",
        "    model_type: str = 'dynamic'\n",
        "    gru_hidden_size: int = 128\n",
        "    gru_num_layers: int = 2\n",
        "    num_attention_heads: int = 4\n",
        "    dropout: float = 0.5\n",
        "    n_classes: int = 2\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training configuration\"\"\"\n",
        "    learning_rate: float = 0.001\n",
        "    weight_decay: float = 1e-4\n",
        "    batch_size: int = 32\n",
        "    num_epochs: int = 30\n",
        "    use_scheduler: bool = True\n",
        "    scheduler_params: Dict = field(default_factory=lambda: {'T_max': 30, 'eta_min': 1e-6})\n",
        "    early_stopping: bool = True\n",
        "    patience: int = 10\n",
        "    min_delta: float = 0.001\n",
        "    grad_clip: Optional[float] = 1.0\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    use_amp: bool = True\n",
        "    device: str = 'cuda'\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    \"\"\"Data loading configuration\"\"\"\n",
        "    # Update these paths for your Kaggle dataset\n",
        "    base_dir: str = '/kaggle/input'  # Kaggle input directory\n",
        "    augmented_dir: str = 'eeg-augmented-datasets'  # Your dataset folder name\n",
        "    datasets: List[str] = field(default_factory=lambda: ['DEAP', 'GAMEEMO', 'SEEDIV'])\n",
        "    train_ratio: float = 0.7\n",
        "    val_ratio: float = 0.15\n",
        "    test_ratio: float = 0.15\n",
        "    balance_classes: bool = True\n",
        "    cache_stft: bool = False\n",
        "    num_workers: int = 2\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Complete experiment configuration\"\"\"\n",
        "    experiment_name: str = \"gru_xnet_kaggle\"\n",
        "    output_dir: str = '/kaggle/working/outputs'  # Kaggle working directory\n",
        "    stft: STFTConfig = field(default_factory=STFTConfig)\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    log_interval: int = 10\n",
        "    seed: int = 42\n",
        "    deterministic: bool = True\n",
        "\n",
        "# Create configuration\n",
        "config = ExperimentConfig()\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(config.output_dir, 'checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(config.output_dir, 'figures'), exist_ok=True)\n",
        "\n",
        "print(\"Configuration created successfully!\")\n",
        "print(f\"Output directory: {config.output_dir}\")\n",
        "print(f\"Datasets: {config.data.datasets}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084058c7",
      "metadata": {},
      "source": [
        "## 3. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe05c9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int, deterministic: bool = True):\n",
        "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_device(preferred_device: str = 'cuda') -> torch.device:\n",
        "    \"\"\"Get device for training\"\"\"\n",
        "    if preferred_device == 'cuda' and torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping callback\"\"\"\n",
        "    def __init__(self, patience: int = 10, min_delta: float = 0.001, mode: str = 'min'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.should_stop = False\n",
        "    \n",
        "    def __call__(self, score: float):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        else:\n",
        "            improved = score < (self.best_score - self.min_delta) if self.mode == 'min' else score > (self.best_score + self.min_delta)\n",
        "            if improved:\n",
        "                self.best_score = score\n",
        "                self.counter = 0\n",
        "            else:\n",
        "                self.counter += 1\n",
        "                if self.counter >= self.patience:\n",
        "                    self.should_stop = True\n",
        "\n",
        "def plot_training_history(history: Dict, save_path: str = None):\n",
        "    \"\"\"Plot training curves\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Loss\n",
        "    if 'loss' in history['train']:\n",
        "        axes[0].plot(history['train']['loss'], label='Train')\n",
        "    if 'loss' in history['val']:\n",
        "        axes[0].plot(history['val']['loss'], label='Val')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "    \n",
        "    # Accuracy\n",
        "    if 'accuracy' in history['train']:\n",
        "        axes[1].plot(history['train']['accuracy'], label='Train')\n",
        "    if 'accuracy' in history['val']:\n",
        "        axes[1].plot(history['val']['accuracy'], label='Val')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Utility functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84578998",
      "metadata": {},
      "source": [
        "## 4. STFT Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600f97ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "class STFTPreprocessor:\n",
        "    \"\"\"STFT transformation for EEG signals\"\"\"\n",
        "    \n",
        "    def __init__(self, sampling_rate: int, nperseg: int = 256, noverlap: int = None,\n",
        "                 nfft: int = None, window: str = 'hann', freq_range: Tuple[float, float] = None):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.nperseg = nperseg\n",
        "        self.noverlap = noverlap if noverlap is not None else nperseg // 2\n",
        "        self.nfft = nfft if nfft is not None else nperseg\n",
        "        self.window = window\n",
        "        self.freq_range = freq_range\n",
        "        \n",
        "        # Calculate frequency indices\n",
        "        self.n_freq_bins = self.nfft // 2 + 1\n",
        "        if self.freq_range is not None:\n",
        "            freq_resolution = self.sampling_rate / self.nfft\n",
        "            self.freq_start_idx = int(self.freq_range[0] / freq_resolution)\n",
        "            self.freq_end_idx = int(self.freq_range[1] / freq_resolution) + 1\n",
        "            self.n_freq_bins = self.freq_end_idx - self.freq_start_idx\n",
        "        else:\n",
        "            self.freq_start_idx = 0\n",
        "            self.freq_end_idx = self.n_freq_bins\n",
        "    \n",
        "    def compute_stft(self, signal_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute STFT for single channel\"\"\"\n",
        "        f, t, Zxx = signal.stft(signal_data, fs=self.sampling_rate, window=self.window,\n",
        "                                nperseg=self.nperseg, noverlap=self.noverlap, nfft=self.nfft)\n",
        "        stft_magnitude = np.abs(Zxx)\n",
        "        return stft_magnitude[self.freq_start_idx:self.freq_end_idx, :]\n",
        "    \n",
        "    def transform(self, eeg_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Transform EEG data (n_channels, n_timepoints) to STFT\"\"\"\n",
        "        n_channels = eeg_data.shape[0]\n",
        "        stft_temp = self.compute_stft(eeg_data[0])\n",
        "        n_freq_bins, n_time_bins = stft_temp.shape\n",
        "        \n",
        "        stft_features = np.zeros((n_channels, n_freq_bins, n_time_bins), dtype=np.float32)\n",
        "        stft_features[0] = stft_temp\n",
        "        \n",
        "        for ch in range(1, n_channels):\n",
        "            stft_features[ch] = self.compute_stft(eeg_data[ch])\n",
        "        \n",
        "        return stft_features\n",
        "\n",
        "class MultiDatasetSTFTPreprocessor:\n",
        "    \"\"\"Handle multiple datasets with different sampling rates\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_configs: Dict, target_freq_bins: int = 129, target_time_bins: int = 126):\n",
        "        self.target_freq_bins = target_freq_bins\n",
        "        self.target_time_bins = target_time_bins\n",
        "        self.preprocessors = {name: STFTPreprocessor(**cfg) for name, cfg in dataset_configs.items()}\n",
        "    \n",
        "    def transform(self, eeg_data: np.ndarray, dataset_name: str, standardize: bool = True) -> np.ndarray:\n",
        "        \"\"\"Transform and standardize STFT output\"\"\"\n",
        "        stft_features = self.preprocessors[dataset_name].transform(eeg_data)\n",
        "        \n",
        "        if standardize:\n",
        "            from scipy.ndimage import zoom\n",
        "            n_channels, curr_freq, curr_time = stft_features.shape\n",
        "            \n",
        "            if curr_freq != self.target_freq_bins or curr_time != self.target_time_bins:\n",
        "                zoom_factors = (1, self.target_freq_bins / curr_freq, self.target_time_bins / curr_time)\n",
        "                stft_features = zoom(stft_features, zoom_factors, order=1)\n",
        "        \n",
        "        return stft_features\n",
        "\n",
        "def create_dataset_stft_configs(target_freq_range: Tuple[float, float] = (0.5, 50.0)) -> Dict:\n",
        "    \"\"\"Create STFT configs for each dataset\"\"\"\n",
        "    return {\n",
        "        'DEAP': {'sampling_rate': 128, 'nperseg': 256, 'freq_range': target_freq_range},\n",
        "        'GAMEEMO': {'sampling_rate': 128, 'nperseg': 256, 'freq_range': target_freq_range},\n",
        "        'SEEDIV': {'sampling_rate': 200, 'nperseg': 400, 'freq_range': target_freq_range}\n",
        "    }\n",
        "\n",
        "print(\"STFT preprocessing classes defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4642233c",
      "metadata": {},
      "source": [
        "## 5. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19aa0fab",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChannelIndependentCNN(nn.Module):\n",
        "    \"\"\"Channel-independent CNN for spatial feature extraction\"\"\"\n",
        "    \n",
        "    def __init__(self, n_freq_bins: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        \n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        \n",
        "        output = self.W_o(attn_output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layer_norm(x + output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "class gru_xnetDynamic(nn.Module):\n",
        "    \"\"\"gru_xnet: CNN-BiGRU-Self Attention Network\"\"\"\n",
        "    \n",
        "    def __init__(self, n_channels: int, n_freq_bins: int, n_time_bins: int, n_classes: int,\n",
        "                 gru_hidden_size: int = 128, num_attention_heads: int = 4, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_channels = n_channels\n",
        "        self.gru_hidden_size = gru_hidden_size\n",
        "        \n",
        "        # Channel-independent CNNs\n",
        "        self.channel_cnns = nn.ModuleList([ChannelIndependentCNN(n_freq_bins) for _ in range(n_channels)])\n",
        "        \n",
        "        # Calculate dimensions after CNN\n",
        "        self.freq_reduced = n_freq_bins // 8\n",
        "        self.time_reduced = n_time_bins // 8\n",
        "        self.feature_dim_per_channel = 128 * self.freq_reduced\n",
        "        \n",
        "        # BiGRU\n",
        "        self.bigru = nn.GRU(n_channels * self.feature_dim_per_channel, gru_hidden_size,\n",
        "                           num_layers=2, batch_first=True, bidirectional=True,\n",
        "                           dropout=dropout if dropout > 0 else 0)\n",
        "        \n",
        "        # Multi-head self-attention\n",
        "        self.attention = MultiHeadSelfAttention(gru_hidden_size * 2, num_attention_heads, dropout)\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(gru_hidden_size * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x_stft):\n",
        "        batch_size = x_stft.shape[0]\n",
        "        actual_channels = x_stft.shape[1]\n",
        "        \n",
        "        # Apply CNNs\n",
        "        channel_features = []\n",
        "        for i in range(min(self.n_channels, actual_channels)):\n",
        "            channel_feat = self.channel_cnns[i](x_stft[:, i:i+1, :, :])\n",
        "            channel_feat = channel_feat.permute(0, 3, 1, 2).reshape(batch_size, self.time_reduced, -1)\n",
        "            channel_features.append(channel_feat)\n",
        "        \n",
        "        # Handle variable channel counts\n",
        "        if actual_channels < self.n_channels:\n",
        "            padding = torch.zeros(batch_size, self.time_reduced,\n",
        "                                (self.n_channels - actual_channels) * self.feature_dim_per_channel,\n",
        "                                device=x_stft.device, dtype=x_stft.dtype)\n",
        "            channel_features.append(padding)\n",
        "        \n",
        "        temporal_features = torch.cat(channel_features, dim=2)\n",
        "        \n",
        "        # BiGRU\n",
        "        gru_output, _ = self.bigru(temporal_features)\n",
        "        \n",
        "        # Attention\n",
        "        attn_output = self.attention(gru_output)\n",
        "        \n",
        "        # Global pooling and classification\n",
        "        pooled = torch.mean(attn_output, dim=1)\n",
        "        output = self.classifier(pooled)\n",
        "        \n",
        "        return output\n",
        "\n",
        "def create_gru_xnet_model(n_channels: int, n_freq_bins: int, n_time_bins: int,\n",
        "                       n_classes: int, **kwargs) -> nn.Module:\n",
        "    \"\"\"Factory function to create model\"\"\"\n",
        "    return gru_xnetDynamic(n_channels, n_freq_bins, n_time_bins, n_classes, **kwargs)\n",
        "\n",
        "print(\"Model architecture defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa1f90d4",
      "metadata": {},
      "source": [
        "## 6. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af6c842",
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Collate function to handle variable channel counts\"\"\"\n",
        "    stft_features_list = [item[0] for item in batch]\n",
        "    labels = torch.stack([item[1] for item in batch])\n",
        "    \n",
        "    max_channels = max([x.shape[0] for x in stft_features_list])\n",
        "    n_freq_bins = stft_features_list[0].shape[1]\n",
        "    n_time_bins = stft_features_list[0].shape[2]\n",
        "    \n",
        "    padded_features = []\n",
        "    for features in stft_features_list:\n",
        "        n_channels = features.shape[0]\n",
        "        if n_channels < max_channels:\n",
        "            padding = torch.zeros(max_channels - n_channels, n_freq_bins, n_time_bins, dtype=features.dtype)\n",
        "            features = torch.cat([features, padding], dim=0)\n",
        "        padded_features.append(features)\n",
        "    \n",
        "    stft_features = torch.stack(padded_features, dim=0)\n",
        "    return stft_features, labels\n",
        "\n",
        "class gru_xnetDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for gru_xnet\"\"\"\n",
        "    \n",
        "    def __init__(self, data: List[np.ndarray], labels: np.ndarray, dataset_names: np.ndarray,\n",
        "                 stft_preprocessor: MultiDatasetSTFTPreprocessor):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.dataset_names = dataset_names\n",
        "        self.stft_preprocessor = stft_preprocessor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        eeg_sample = self.data[idx]\n",
        "        dataset_name = self.dataset_names[idx]\n",
        "        stft_features = self.stft_preprocessor.transform(eeg_sample, dataset_name, standardize=True)\n",
        "        stft_features = torch.from_numpy(stft_features).float()\n",
        "        label = torch.tensor(self.labels[idx]).long()\n",
        "        return stft_features, label\n",
        "\n",
        "def load_augmented_dataset(base_dir: str, dataset_name: str) -> Tuple:\n",
        "    \"\"\"Load augmented dataset from pickle files\"\"\"\n",
        "    # Adjust this function based on your Kaggle dataset structure\n",
        "    dataset_path = os.path.join(base_dir, f'{dataset_name}_augmented')\n",
        "    \n",
        "    # Load your augmented data\n",
        "    # Adjust file paths based on actual structure\n",
        "    with open(os.path.join(dataset_path, 'data.pkl'), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    with open(os.path.join(dataset_path, 'labels.pkl'), 'rb') as f:\n",
        "        labels = pickle.load(f)\n",
        "    with open(os.path.join(dataset_path, 'subjects.pkl'), 'rb') as f:\n",
        "        subjects = pickle.load(f)\n",
        "    \n",
        "    return data, labels, subjects\n",
        "\n",
        "def create_data_loaders(config: ExperimentConfig) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    \"\"\"Create train/val/test data loaders\"\"\"\n",
        "    print(\"Loading datasets...\")\n",
        "    \n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_dataset_names = []\n",
        "    \n",
        "    augmented_dir = os.path.join(config.data.base_dir, config.data.augmented_dir)\n",
        "    \n",
        "    for dataset_name in config.data.datasets:\n",
        "        print(f\"Loading {dataset_name}...\")\n",
        "        data, labels, subjects = load_augmented_dataset(augmented_dir, dataset_name)\n",
        "        \n",
        "        all_data.append(data)\n",
        "        all_labels.append(labels)\n",
        "        all_dataset_names.extend([dataset_name] * len(labels))\n",
        "        print(f\"  Loaded {len(labels)} samples\")\n",
        "    \n",
        "    # Combine\n",
        "    labels = np.concatenate(all_labels)\n",
        "    dataset_names = np.array(all_dataset_names)\n",
        "    \n",
        "    # Split data\n",
        "    n_samples = len(labels)\n",
        "    indices = np.random.permutation(n_samples)\n",
        "    \n",
        "    n_train = int(n_samples * config.data.train_ratio)\n",
        "    n_val = int(n_samples * config.data.val_ratio)\n",
        "    \n",
        "    train_idx = indices[:n_train]\n",
        "    val_idx = indices[n_train:n_train + n_val]\n",
        "    test_idx = indices[n_train + n_val:]\n",
        "    \n",
        "    # Extract data for splits\n",
        "    def extract_split(split_indices):\n",
        "        split_data = []\n",
        "        for i in split_indices:\n",
        "            # Find dataset and local index\n",
        "            dataset_idx = 0\n",
        "            local_idx = i\n",
        "            cumsum = 0\n",
        "            for j, data_array in enumerate(all_data):\n",
        "                if i < cumsum + len(data_array):\n",
        "                    dataset_idx = j\n",
        "                    local_idx = i - cumsum\n",
        "                    break\n",
        "                cumsum += len(data_array)\n",
        "            split_data.append(all_data[dataset_idx][local_idx])\n",
        "        return split_data, labels[split_indices], dataset_names[split_indices]\n",
        "    \n",
        "    train_data, train_labels, train_datasets = extract_split(train_idx)\n",
        "    val_data, val_labels, val_datasets = extract_split(val_idx)\n",
        "    test_data, test_labels, test_datasets = extract_split(test_idx)\n",
        "    \n",
        "    print(f\"\\nSplit: Train={len(train_labels)}, Val={len(val_labels)}, Test={len(test_labels)}\")\n",
        "    \n",
        "    # Create STFT preprocessor\n",
        "    stft_configs = create_dataset_stft_configs(config.stft.freq_range)\n",
        "    stft_preprocessor = MultiDatasetSTFTPreprocessor(stft_configs, config.stft.target_freq_bins,\n",
        "                                                     config.stft.target_time_bins)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = gru_xnetDataset(train_data, train_labels, train_datasets, stft_preprocessor)\n",
        "    val_dataset = gru_xnetDataset(val_data, val_labels, val_datasets, stft_preprocessor)\n",
        "    test_dataset = gru_xnetDataset(test_data, test_labels, test_datasets, stft_preprocessor)\n",
        "    \n",
        "    # Create loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True,\n",
        "                             num_workers=config.data.num_workers, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False,\n",
        "                           num_workers=config.data.num_workers, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False,\n",
        "                            num_workers=config.data.num_workers, collate_fn=custom_collate_fn)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "print(\"Data loading functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44b67d3",
      "metadata": {},
      "source": [
        "## 7. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6939d994",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Training manager\"\"\"\n",
        "    \n",
        "    def __init__(self, config: ExperimentConfig):\n",
        "        self.config = config\n",
        "        set_seed(config.seed, config.deterministic)\n",
        "        self.device = get_device(config.training.device)\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        # Create data loaders\n",
        "        self.train_loader, self.val_loader, self.test_loader = create_data_loaders(config)\n",
        "        \n",
        "        # Get dimensions\n",
        "        sample_batch = next(iter(self.train_loader))\n",
        "        n_channels, n_freq_bins, n_time_bins = sample_batch[0].shape[1:]\n",
        "        print(f\"\\nData dimensions: channels={n_channels}, freq={n_freq_bins}, time={n_time_bins}\")\n",
        "        \n",
        "        # Create model\n",
        "        self.model = create_gru_xnet_model(\n",
        "            n_channels=n_channels, n_freq_bins=n_freq_bins, n_time_bins=n_time_bins,\n",
        "            n_classes=config.model.n_classes, gru_hidden_size=config.model.gru_hidden_size,\n",
        "            num_attention_heads=config.model.num_attention_heads, dropout=config.model.dropout\n",
        "        ).to(self.device)\n",
        "        \n",
        "        n_params = sum(p.numel() for p in self.model.parameters())\n",
        "        print(f\"Model parameters: {n_params:,}\")\n",
        "        \n",
        "        # Training components\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.training.learning_rate,\n",
        "                                   weight_decay=config.training.weight_decay)\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer,\n",
        "                                                              **config.training.scheduler_params)\n",
        "        self.scaler = GradScaler() if config.training.use_amp else None\n",
        "        self.early_stopping = EarlyStopping(config.training.patience, config.training.min_delta)\n",
        "        \n",
        "        self.history = {'train': {'loss': [], 'accuracy': []}, 'val': {'loss': [], 'accuracy': []}}\n",
        "        self.best_val_loss = float('inf')\n",
        "    \n",
        "    def train_epoch(self, epoch: int):\n",
        "        \"\"\"Train one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.training.num_epochs}\")\n",
        "        \n",
        "        for batch_idx, (stft_features, labels) in enumerate(pbar):\n",
        "            stft_features, labels = stft_features.to(self.device), labels.to(self.device)\n",
        "            \n",
        "            if self.scaler:\n",
        "                with autocast():\n",
        "                    outputs = self.model(stft_features)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    loss = loss / self.config.training.gradient_accumulation_steps\n",
        "                \n",
        "                self.scaler.scale(loss).backward()\n",
        "                \n",
        "                if (batch_idx + 1) % self.config.training.gradient_accumulation_steps == 0:\n",
        "                    if self.config.training.grad_clip:\n",
        "                        self.scaler.unscale_(self.optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
        "                                                      self.config.training.grad_clip)\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                    self.optimizer.zero_grad()\n",
        "            else:\n",
        "                outputs = self.model(stft_features)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss = loss / self.config.training.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "                \n",
        "                if (batch_idx + 1) % self.config.training.gradient_accumulation_steps == 0:\n",
        "                    if self.config.training.grad_clip:\n",
        "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
        "                                                      self.config.training.grad_clip)\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += loss.item() * self.config.training.gradient_accumulation_steps\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({'loss': total_loss / (batch_idx + 1), 'acc': 100. * correct / total})\n",
        "        \n",
        "        return total_loss / len(self.train_loader), 100. * correct / total\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def validate(self):\n",
        "        \"\"\"Validate\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for stft_features, labels in tqdm(self.val_loader, desc=\"Validation\"):\n",
        "            stft_features, labels = stft_features.to(self.device), labels.to(self.device)\n",
        "            outputs = self.model(stft_features)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        return total_loss / len(self.val_loader), 100. * correct / total\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Starting Training\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        \n",
        "        for epoch in range(self.config.training.num_epochs):\n",
        "            train_loss, train_acc = self.train_epoch(epoch)\n",
        "            val_loss, val_acc = self.validate()\n",
        "            \n",
        "            self.history['train']['loss'].append(train_loss)\n",
        "            self.history['train']['accuracy'].append(train_acc)\n",
        "            self.history['val']['loss'].append(val_loss)\n",
        "            self.history['val']['accuracy'].append(val_acc)\n",
        "            \n",
        "            print(f\"\\nEpoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%\")\n",
        "            print(f\"           Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
        "            \n",
        "            # Save best model\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                torch.save(self.model.state_dict(),\n",
        "                          os.path.join(config.output_dir, 'checkpoints', 'best_model.pth'))\n",
        "                print(\"  Saved best model\")\n",
        "            \n",
        "            self.scheduler.step()\n",
        "            \n",
        "            # Early stopping\n",
        "            self.early_stopping(val_loss)\n",
        "            if self.early_stopping.should_stop:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Training Complete!\")\n",
        "        print(\"=\"*60)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def test(self):\n",
        "        \"\"\"Test on test set\"\"\"\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load(\n",
        "            os.path.join(config.output_dir, 'checkpoints', 'best_model.pth')))\n",
        "        self.model.eval()\n",
        "        \n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        \n",
        "        for stft_features, labels in tqdm(self.test_loader, desc=\"Testing\"):\n",
        "            stft_features, labels = stft_features.to(self.device), labels.to(self.device)\n",
        "            outputs = self.model(stft_features)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "        \n",
        "        test_loss = total_loss / len(self.test_loader)\n",
        "        test_acc = 100. * correct / total\n",
        "        \n",
        "        print(f\"\\nTest Results:\")\n",
        "        print(f\"  Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Accuracy: {test_acc:.2f}%\")\n",
        "        \n",
        "        return test_loss, test_acc, np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "print(\"Trainer class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5fc88d",
      "metadata": {},
      "source": [
        "## 8. Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100a73bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seed\n",
        "set_seed(config.seed, config.deterministic)\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(config)\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(config.output_dir, 'history.json'), 'w') as f:\n",
        "    json.dump(trainer.history, f, indent=4)\n",
        "\n",
        "# Plot training curves\n",
        "plot_training_history(trainer.history, \n",
        "                     save_path=os.path.join(config.output_dir, 'figures', 'training_curves.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1075b060",
      "metadata": {},
      "source": [
        "## 9. Test and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c308b18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test\n",
        "test_loss, test_acc, predictions, labels = trainer.test()\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f'Confusion Matrix (Acc: {test_acc:.2f}%)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig(os.path.join(config.output_dir, 'figures', 'confusion_matrix.png'), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=['Negative', 'Positive']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83298363",
      "metadata": {},
      "source": [
        "## 10. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4593568f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final results\n",
        "results = {\n",
        "    'test_loss': float(test_loss),\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'config': {\n",
        "        'batch_size': config.training.batch_size,\n",
        "        'learning_rate': config.training.learning_rate,\n",
        "        'num_epochs': config.training.num_epochs,\n",
        "        'datasets': config.data.datasets\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(config.output_dir, 'results.json'), 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(\"\\nResults saved!\")\n",
        "print(f\"Output directory: {config.output_dir}\")\n",
        "print(f\"Best model: {os.path.join(config.output_dir, 'checkpoints', 'best_model.pth')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
